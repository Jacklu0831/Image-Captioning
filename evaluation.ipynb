{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"image_captioning_eval.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xA8IXK9xCXsI","colab_type":"text"},"source":["# Image Captioning\n","\n","Keywords: CV, NLP, LSTM, attention (soft), MS-COCO, image processing, InceptionV3.\n","\n","Implementation similar to [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)."]},{"cell_type":"code","metadata":{"id":"mJkkfrpz9Qz7","colab_type":"code","colab":{}},"source":["# miscellaneous\n","import numpy as np\n","from collections import Counter\n","from h5py\n","from json\n","import time\n","from tqdm import tqdm\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","# code\n","from caption import *\n","\n","# PyTorch\n","import torch\n","import torch.optim\n","import torch.utils.data\n","import torch.backends.cudnn as cudnn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","import torchvision.transforms as transforms\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","# computing environment\n","assert torch.cuda.is_available()\n","device = torch.device('cuda')\n","cudnn.benchmark = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gi02pqT2B4b2","colab_type":"text"},"source":["### Parameters"]},{"cell_type":"code","metadata":{"id":"UDGCpUvP9aVf","colab_type":"code","colab":{}},"source":["input_dir = 'processed_input' # input directory (processed data)\n","trained_model = \"final_model.pth.tar\"  # pre-trained model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"34kp0OLiByYX","colab_type":"text"},"source":["### Load Word Map and Model"]},{"cell_type":"code","metadata":{"id":"Xr0wF-yX_GFi","colab_type":"code","colab":{}},"source":["# load word map\n","with open('drive/My Drive/wordmap.json', 'r') as f:\n","    word_map = json.load(f)\n","# reverse the word map\n","reverse_word_map = {v:k for k,v in word_map.items()}\n","vocab_size = len(word_map)\n","\n","# load model with state dict (only need enc and dec)\n","trained_model = torch.load(trained_model)\n","decoder = trained_model['decoder']\n","encoder = trained_model['encoder']\n","\n","# set up dec and enc\n","encoder = encoder.to(device)\n","decoder = decoder.to(device)\n","encoder.eval()\n","decoder.eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_B4euZCCNoB","colab_type":"text"},"source":["### Transform"]},{"cell_type":"code","metadata":{"id":"NwfrGnVLBx8R","colab_type":"code","colab":{}},"source":["normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SO92cxOUCROf","colab_type":"text"},"source":["### Evaluate Function"]},{"cell_type":"code","metadata":{"id":"UP-QeOVLCS7v","colab_type":"code","colab":{}},"source":["def evaluate(b):\n","    data_loader = data.DataLoader(CapData(input_dir, 'test', transform=transforms.Compose([normalize]),\n","                                 batch_size=1, shuffle=True, num_workers=1, pin_memory=True))\n","    \n","    ground_truths = []\n","    predictions = []\n","    \n","    for i, (img, caps, len_caps, all_caps) in enumerate(tqdm(loader)):\n","        img = img.to(device)\n","        \n","        # get flattened encoding then expand it for beam search\n","        enc_out = encoder(img)\n","        enc_size = enc_out.size(1)\n","        enc_dim = enc_out.size(3)\n","        enc_out = enc_out.view(1, -1, enc_dim)\n","        num_pix = enc_out.size(1)\n","        enc_out = enc_out.expand(b, num_pix, enc_dim)\n","                                 \n","        # initialize top previous words, favored sequences, and scores\n","        prev_words = torch.LongTensor([[word_map['<start>']]] * b).to(device)\n","        seqs = prev_words\n","        top_scores = torch.zeros(b, 1).to(device)\n","        \n","        # containers of current seqs their scores\n","        curr_seqs = []\n","        curr_scores = []\n","        \n","        # init timestamp and LSTM internal states\n","        t = 1\n","        hidden, cell = decoder.init_state()\n","        \n","        while True:\n","            # decode with attention procedure\n","            emb = decoder.embedding(prev_words).squeeze(1)\n","            att_out, _ = decoder.attention(enc_out, hidden)\n","            gate = decoder.sigmoid(decoder.f_beta(hidden))\n","            att_out = gate * att_out\n","            hidden, cell = decoder.decode(torch.cat([emb, att_out], dim=1), (hidden, cell))\n","            \n","            # get scores\n","            scores = decoder.fc(hidden)\n","            scores = F.log_softmax(scores, dim=1)\n","            scores = top_scores.expand_as(scores) + scores\n","            \n","            # get top scores and words for this sequence step\n","            if t == 1:\n","                top_scores, top_words = scores[0].topk(b, 0, largest=True, sorted=True)\n","            else:\n","                top_scores, top_words = scores.view(-1).topk(b, 0, largest=True, sorted=True)\n","            \n","            # get indices to add new words\n","            prev_word_idxs = top_words / vocab_size\n","            next_word_idxs = top_words % vocab_size\n","            seqs = torch.cat([seqs[prev_word_idxs], next_word_idxs.unsqueeze(1)], dim=1)\n","            \n","            # get sequences that reached and have not reached <end>\n","            going_idxs = [idx for idx, next_word in enumerate(next_word_idxs) if next_word is not word_map['end']]\n","            ended_idxs = [set(range(len(next_word_idxs)) - set(going_idxs))]\n","            \n","            # store ended sequences\n","            if ended_idxs:\n","                curr_seqs.extend(seqs[ended_idxs].tolist())\n","                curr_scores.extend(top_scores[ended_idxs])\n","                \n","            # adjust beam size\n","            b -= len(ended_idxs)\n","            if b == 0:\n","                break\n","                \n","            # get rid of completed\n","            seqs = seqs[going_idxs]\n","            hidden = hidden[prev_word_idxs[going_idxs]]\n","            cell = cell[prev_word_idxs[going_idxs]]\n","            enc_out = enc_out[prev_word_idxs[going_idxs]]\n","            top_scores = top_scores[going_idxs].unsqueeze(1)\n","            prev_words = next_word_idxs[going_idxs].unsqueeze(1)\n","            \n","            # update step\n","            if t > 100:\n","                break\n","            t += 1\n","        \n","        # get seq\n","        i = curr_scores.index(max(curr_scores))\n","        seq = curr_seqs[i]\n","            \n","        # get ground truth and prediction\n","        img_caps = all_caps[0].tolist()\n","        img_caps = list(map(lambda cap: [w for w in cap if (w != word_map['<start>'] and w != word_map['<pad>'])]), \n","                        img_caps)\n","        ground_truths.append(img_caps)\n","        predictions.append([w for w in seq if (w != word_map['<start>'] and w != word_map['<end>'])])\n","        \n","        assert len(ground_truths) == len(predictions)\n","    \n","    # compute bleu scores\n","    bleu_scores = corpus_bleu(ground_truths, predictions)\n","    \n","    return bleu_scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rB24YGO3D4ey","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}